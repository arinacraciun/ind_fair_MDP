# config.yaml - The Central Control Panel for the Fairness RL Experiment

###File and Path Settings
file_paths:
  input_data: "cleaned_adult.csv"
  metrics_output_csv: "metrics_output.csv"
  plot_output_image: "plot_one_run.png"

###Dataset and Feature Specification
# Define all column names and roles from your dataset here.
dataset_columns:
  target_original: 'income'            # The original target column in the CSV
  target_positive_class: '>50K'        # The value that represents the "positive" outcome (for binarization)
  sensitive_attribute: 'sex'           # The column name for the sensitive group
  
  # List all features that the agent's actions can change over time.
  driftable_features:
    - 'capital.gain'
    - 'capital.loss'

### Drift Logic
# This section defines exactly how driftable features change based on agent actions.
# This makes the drift mechanism completely generic.
drift_logic:
  # For action '0'
  0:
    feature_to_drift: 'capital.loss'   # Which feature to change
    value: 5.0                         # The percentage or value to use
  # For action '1'
  1:
    feature_to_drift: 'capital.gain'
    value: 5.0

### Reward structure for the environment
rewards:
  terminal_reward: 100.0
  terminal_penalty: -100.0
  step_cost: 1.0

### Model & Training Hyperparameters
training:
  seed: 42
  total_timesteps: 30000
  device: 'auto' # 'auto', 'cpu', 'cuda'
  max_episode_steps: 30

# K-Nearest Neighbors settings for finding similar pairs
knn_settings:
  k_neighbors: 5  # The number of neighbors to find in the state space.
  sigma: 1.0      # The sigma value used for calculating similarity on states.

# FairDQN specific hyperparameters
fair_dqn_params:
  lambda_fair: 5.0             # The strength of the fairness penalty. A higher value forces the agent to prioritize fairness more.
  batch_size_pairs: 128         # The number of similar pairs to sample from `fair_pairs_list` for each training update.
  weighted_frac: 0.0            # The fraction of the `batch_size_pairs` that are sampled based on similarity (more similar pairs are more likely to be picked).
  learning_rate: 0.001          # Controls how large of a step the optimizer takes during training. A smaller value leads to slower but potentially more stable learning.
  buffer_size: 100000           # The maximum number of recent experiences (state, action, reward, next_state) to store in the replay buffer.
  learning_starts: 10000         # The number of random steps to take to fill the buffer before the agent starts learning. This ensures initial training batches are diverse.
  batch_size: 256               # The number of experiences to sample from the replay buffer for each training update.
  train_freq: 1                 # How often to update the model. A value of 1 means one training update is performed after every single step in the environment.
  gradient_steps: 1             # The number of gradient descent updates to perform each time `train_freq` is met.
  target_update_interval: 1000  # The frequency (in timesteps) at which the weights of the "target network" are updated. This is a key technique for stabilizing DQN training.
  exploration_fraction: 0.2     # The fraction of the total training time over which the exploration rate (epsilon) will linearly decrease from its initial to its final value.
  exploration_initial_eps: 1.0  # The starting value for epsilon. A value of 1.0 means the agent starts by taking completely random actions to explore the environment.
  exploration_final_eps: 0.05   # The final, minimum value for epsilon. The agent will always have at least a 5% chance of taking a random action, which prevents it from getting stuck.
  tau: 0.005                    # The "soft update" parameter for the target network. A value of 1.0 means it's a "hard update" (a direct copy), which is standard with `target_update_interval`.
  gamma: 0.99                   # The discount factor for future rewards. A value close to 1 (like 0.99) makes the agent prioritize long-term rewards.

### Evaluation Settings 
evaluation:
  eval_freq: 500                      # Run this entire evaluation process every 500 training steps.
  n_eval_episodes: 100                # Number of episodes to run to get a stable measure of the agent's reward
