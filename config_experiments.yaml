###  File and Path Settings 
file_paths:
  input_data: "cleaned_adult.csv"
  metrics_output_dir: "experiments/metrics/"
  plots_output_dir: "experiments/plots/"

###  Dataset and Feature Specification 
# Define all column names and roles from your dataset here.
dataset_columns:
  target_original: 'income'            # The original target column in the CSV
  target_positive_class: '>50K'        # The value that represents the "positive" outcome (for binarization)
  sensitive_attribute: 'sex'           # The column name for the sensitive group
  
  # List all features that the agent's actions can change over time.
  driftable_features:
    - 'capital.gain'
    - 'capital.loss'

###  Drift Logic (DEFAULT SETTING) 
drift_logic:
  0:
    feature_to_drift: 'capital.loss'
    value: 5.0 
  1:
    feature_to_drift: 'capital.gain'
    value: 5.0

# Reward structure
rewards:
  terminal_reward: 100.0
  terminal_penalty: -100.0
  step_cost: 1.0

###  Model & Training Hyperparameters 
training:
  seeds: [42, 43, 44] # Define seeds here
  total_timesteps: 25000
  device: 'auto'
  max_episode_steps: 10

# K-Nearest Neighbors settings for finding similar pairs
knn_settings:
  k_neighbors: 5  # The number of neighbors to find in the state space.
  sigma: 1.0      # The sigma value used for calculating similarity on states.

###  Evaluation Settings 
evaluation:
  eval_freq: 500                      # Run this entire evaluation process every 500 training steps.
  n_eval_episodes: 50                 # Number of episodes to run to get a stable measure of the agent's reward.


# FairDQN specific hyperparameters
fair_dqn_params:
  learning_rate: 0.001          # Controls how large of a step the optimizer takes during training. A smaller value leads to slower but potentially more stable learning.
  buffer_size: 100000           # The maximum number of recent experiences (state, action, reward, next_state) to store in the replay buffer.
  learning_starts: 5000         # The number of random steps to take to fill the buffer before the agent starts learning. This ensures initial training batches are diverse.
  batch_size: 256               # The number of experiences to sample from the replay buffer for each training update.
  train_freq: 1                 # How often to update the model. A value of 1 means one training update is performed after every single step in the environment.
  gradient_steps: 1             # The number of gradient descent updates to perform each time `train_freq` is met.
  target_update_interval: 1000  # The frequency (in timesteps) at which the weights of the "target network" are updated. This is a key technique for stabilizing DQN training.
  exploration_fraction: 0.1     # The fraction of the total training time over which the exploration rate (epsilon) will linearly decrease from its initial to its final value.
  exploration_initial_eps: 1.0  # The starting value for epsilon. A value of 1.0 means the agent starts by taking completely random actions to explore the environment.
  exploration_final_eps: 0.05   # The final, minimum value for epsilon. The agent will always have at least a 5% chance of taking a random action, which prevents it from getting stuck.
  tau: 0.005                    # The "soft update" parameter for the target network. A value of 1.0 means it's a "hard update" (a direct copy), which is standard with `target_update_interval`.
  gamma: 0.99                   # The discount factor for future rewards. A value close to 1 (like 0.99) makes the agent prioritize long-term rewards.

###  Experiment Definition Section 
# This new section tells the runner script which parameters to sweep.
experiment_sweeps:
  # The key is the exact path to the parameter in this config file.
  # The value is a list of settings to test.
  
  fair_dqn_params.lambda_fair: [1.0, 5.0, 10.0]
  fair_dqn_params.batch_size_pairs: [32, 128]
  fair_dqn_params.weighted_frac: [0.0, 0.5, 1.0]

  # For complex parameters like drift, we define a list of named objects.
  drift_logic:
    - name: "no_drift" # A friendly name for the file output
      value:
        0:
          feature_to_drift: 'capital.loss'
          value: 0.0
        1:
          feature_to_drift: 'capital.gain'
          value: 0.0
          
    - name: "low_drift"
      value:
        0:
          feature_to_drift: 'capital.loss'
          value: 5.0
        1:
          feature_to_drift: 'capital.gain'
          value: 5.0
    
    - name: "medium_drift"
      value:
        0:
          feature_to_drift: 'capital.loss'
          value: 10.0
        1:
          feature_to_drift: 'capital.gain'
          value: 10.0
    
    - name: "high_drift"
      value:
        0:
          feature_to_drift: 'capital.loss'
          value: 20.0
        1:
          feature_to_drift: 'capital.gain'
          value: 20.0
